{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python370jvsc74a57bd006151ea8e193c97c9be0de8c86907398aeeee9a6cb9ce34c6620f7a8e1848560",
   "display_name": "Python 3.7.0 64-bit ('deepfake': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "所有的都在git上面有官方的torch和caffe的實現\n",
    " \n",
    "這裡只能根據自己的一個粗淺的理解，梳理一個簡單的代碼版本\n",
    "\n",
    "略去了並行處理和一些複雜的優化，包括各個層獨特的優化以及使用GPU train\n",
    "\n",
    "[官方實現](https://github.com/facebookresearch/dlrm)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class DLRM_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLRM_Net,self).__init__()\n",
    "        self.emb_l = self.create_emb(2,[4,3,2])\n",
    "        self.bot_l = self.create_mlp([4,3,2],-1)\n",
    "        self.top_l = self.create_mlp([8,4,2,1],2)\n",
    "        self.loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "    def create_mlp(self,ln,sigmoid_layer):\n",
    "        layers = nn.ModuleList()\n",
    "        for i in range(0,len(ln)-1):\n",
    "            n = ln[i]\n",
    "            m = ln[i+1]\n",
    "            LL = nn.Linear(int(n),int(m),bias=True)\n",
    "            layers.append(LL)\n",
    "            if i==sigmoid_layer:\n",
    "                layers.append(nn.Sigmoid())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "        return torch.nn.Sequential(*layers)\n",
    "    def create_emb(self,m,ln):\n",
    "        emb_l = nn.ModuleList()\n",
    "        for i in range(0,len(ln)):\n",
    "            n = ln[i]\n",
    "            EE = nn.EmbeddingBag(n,m,mode=\"sum\",sparse=True)\n",
    "            emb_l.append(EE)\n",
    "        return emb_l\n",
    "    def apply_emb(self,lS_o,lS_i,emb_l):\n",
    "        ly = []\n",
    "        for k,sparse_index_group_batch in enumerate(lS_i):\n",
    "            sparse_offset_group_batch = lS_o[k]\n",
    "            E = emb_l[k]\n",
    "            V = E(\n",
    "                sparse_index_group_batch,\n",
    "                sparse_offset_group_batch,\n",
    "            )\n",
    "            ly.append(V)\n",
    "        return ly\n",
    "    def apply_mlp(self,x,layers):\n",
    "        return layers(x)\n",
    "    \n",
    "    def interact_features(self,x,ly):\n",
    "        (batch_size,d) = x.shape\n",
    "        T = torch.cat([x]+ly,dim=1).view((batch_size,-1,d))\n",
    "        Z = torch.bmm(T,torch.transpose(T,1,2))\n",
    "        _,ni,nj = Z.shape\n",
    "        li = torch.tensor([i for i in range(ni) for j in range(i)])\n",
    "        lj = torch.tensor([j for i in range(nj) for j in range(i)])\n",
    "        Zflat = Z[:, li, lj]\n",
    "        # concatenate dense features and interactions\n",
    "        R = torch.cat([x] + [Zflat], dim=1)\n",
    "        return R\n",
    "\n",
    "    def forward(self,dense_x,lS_o,lS_i):\n",
    "        x = self.apply_mlp(dense_x,self.bot_l)\n",
    "        ly = self.apply_emb(lS_o,lS_i,self.emb_l)\n",
    "        z = self.interact_features(x,ly)\n",
    "        p = self.apply_mlp(z,self.top_l)\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[tensor([ 0,  2,  4,  5,  7,  9, 10, 12, 13, 16, 17, 18, 19, 20, 22, 24, 26, 29,\n        32, 34]), tensor([ 0,  1,  2,  3,  4,  5,  6,  8,  9, 11, 13, 14, 17, 19, 21, 22, 24, 26,\n        27, 28]), tensor([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19,\n        20, 21])]\n[tensor([0, 2, 1, 2, 3, 0, 1, 2, 3, 1, 0, 2, 3, 0, 2, 3, 3, 0, 2, 2, 0, 1, 0, 3,\n        1, 2, 1, 2, 3, 0, 1, 2, 1, 3, 3]), tensor([2, 2, 1, 1, 1, 0, 1, 2, 1, 1, 2, 0, 1, 1, 0, 1, 2, 0, 1, 0, 1, 2, 0, 2,\n        1, 2, 0, 0, 2]), tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "# here we only show the fake train code and the output and input shape\n",
    "train code\n",
    "parameters = dlrm.parameters()\n",
    "optimizer = torch.optimi.SGD(parameters,lr=0.01)\n",
    "for j,inputBatch in enumerate(train_ld):\n",
    "    x,lS_o,lS_i,T = inputBatch\n",
    "    # forward\n",
    "    Z = dlrm(\n",
    "        x,\n",
    "        lS_o,\n",
    "        lS_i\n",
    "    )\n",
    "    E = dlrm.loss_fn(Z,T)\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    E.backward()\n",
    "'''\n",
    "# fake data described here\n",
    "# dense feature\n",
    "B = 20\n",
    "m_den= 4\n",
    "ln_emb = [4,3,2]\n",
    "num_indices_per_lookup = 10\n",
    "Xt = torch.tensor(np.random.rand(B, m_den).astype(np.float32))\n",
    "# sparse feature (sparse indices)\n",
    "lS_emb_offsets = []\n",
    "lS_emb_indices = []\n",
    "# for each embedding generate a list of n lookups,\n",
    "# where each lookup is composed of multiple sparse indices\n",
    "for size in ln_emb:\n",
    "    lS_batch_offsets = []\n",
    "    lS_batch_indices = []\n",
    "    offset = 0\n",
    "    for _ in range(B):\n",
    "        # num of sparse indices to be used per embedding (between\n",
    "        # random between [1,num_indices_per_lookup])\n",
    "        r = np.random.random(1)\n",
    "        sparse_group_size = np.int64(\n",
    "            np.round(max([1.0], r * min(size, num_indices_per_lookup)))\n",
    "        )\n",
    "        # sparse indices to be used per embedding\n",
    "        r = np.random.random(sparse_group_size)\n",
    "        sparse_group = np.unique(np.round(r * (size - 1)).astype(np.int64))\n",
    "        # reset sparse_group_size in case some index duplicates were removed\n",
    "        sparse_group_size = np.int32(sparse_group.size)\n",
    "        # store lengths and indices\n",
    "        lS_batch_offsets += [offset]\n",
    "        lS_batch_indices += sparse_group.tolist()\n",
    "        # update offset for next iteration\n",
    "        offset += sparse_group_size\n",
    "    lS_emb_offsets.append(torch.tensor(lS_batch_offsets))\n",
    "    lS_emb_indices.append(torch.tensor(lS_batch_indices))\n",
    "print(lS_emb_offsets)\n",
    "print(lS_emb_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.5473],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5448],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5431],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5466],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5473],\n",
       "        [0.5473]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "dlrm = DLRM_Net()\n",
    "Z_test = dlrm(Xt,lS_emb_offsets,lS_emb_indices)\n",
    "Z_test"
   ]
  }
 ]
}